{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Team List and Credits:\n",
    "- Ricardo Aguiar Bomeny: \n",
    "- Thiago Donato: \n",
    "- Darren Rauvola: Analysis, Conceptualization, Experimental investigation, Writing - original draft \n",
    "- Kiame McCartha:\n",
    "- Dmitri Singer: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these three categories of features: temporal features (hour, day of week, holidays), spatial features (pickup zone), and weather conditions (temperature, precipitation, snowfall) which one feature or combination of features is most important in forecasting taxi demand (number of rides per time interval - daily or hourly) in New York City?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxi services are a key part of NYC’s urban lifestyle. The iconic yellow cabs completed over 200 million trips annually before the pandemic <a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1). Surprisingly, we found out that the NYC Taxi and Limousine Commission (TLC) documents their trips extensively since 2009. This makes it into one of the largest and most thorough urban mobility datasets in the world. The data includes features like pickup and dropoff locations, timestamps, fare information, payment type, and passenger counts for every metered taxi ride. We were blown away when we saw this! There were almost infinite questions we could ask or investigate. We thought of predicting taxi demand because it is a key component for companies that want to optimize fleet allocation, drivers who want to be in high-demand areas, and city planners so they can manage transportation policy based on demand.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2)\n",
    "\n",
    "Previous research has shown that taxi demand follows predictable spatiotemporal patterns. Moreira-Matias et al. (2013) investigated how time-series forecasting methods work for capturing hourly and daily demand cycles. They found that temporal features such as hour of day and day of week were one of the strongest predictors of taxi pickups <a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3).\n",
    "\n",
    "Besides this, inspired by recent extreme weather in NYC, we thought about investigating whether weather conditions impact taxi ridership patterns. Singhal et al. (2014) did a comprehensive analysis of weather effects on NYC rides and found that precipitation has a statistically significant positive effect on taxi demand (as rain increases, more people choose taxis over walking or waiting for buses) <a name=\"cite_ref-4\"></a>[<sup>4</sup>](#cite_note-4). The same was true for extreme heat.\n",
    "\n",
    "Our project builds on this prior work by developing an integrated prediction model that combines temporal features (hour, day of week, month), spatial features (pickup zone), and weather conditions (temperature, precipitation) to forecast taxi demand + fare distribution. While previous studies have looked at these features separately, we’ll try to quantify their relative importance using feature importance analysis and compare the predictive performance of multiple machine learning models including Linear Regression, Random Forest, and maybe even neural networks. \n",
    "\n",
    "Using recent TLC trip data and NOAA weather observations from Central Park, we will evaluate whether the patterns identified in earlier research hold in contemporary data and identify which factors most strongly drive taxi demand in New York City.\n",
    "\n",
    "Citations:\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) NYC Taxi and Limousine Commission. (2024). TLC Trip Record Data. https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Correa, D. & Moyano, C. (2023). Analysis and prediction of New York City taxi and Uber demands. Journal of Applied Research and Technology, 21(5), 886-898.\n",
    "\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3) Moreira-Matias, L., Gama, J., Ferreira, M., Mendes-Moreira, J., & Damas, L. (2013). Predicting taxi-passenger demand using streaming data. IEEE Transactions on Intelligent Transportation Systems, 14(3), 1393-1402.\n",
    "\n",
    "4. <a name=\"cite_note-4\"></a> [^](#cite_ref-4) Singhal, A., Kamga, C., & Yazici, A. (2014). Impact of weather on urban transit ridership. Transportation Research Part A: Policy and Practice, 69, 379-391.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temporal feature relevance: We believe that temporal features like hour of day and holidays will be most correlated with taxi demand because of things like “rush hour” and from our personal experience trying to get a taxi/uber during valentines at dinner time for example is much harder than on non-holiday dates. We don’t think day of the week plays a significant role as much as these other temporal features because in a busy city like NY, the variability of the things people do in a given day is too high to be able to claim that there is a special day of the week where demand is higher. We think demand per day will average out to be about the same.\n",
    "- Spacial feature relevance: We believe that some burrows will have much higher demand than others simply because of the functional role of that burrow (eg. living spaces vs work spaces vs social spaces), with Manhattan (for its social/work areas) being the most busy.\n",
    "- Weather feature relevance: We think extreme temperatures (<50F and >90F), high precipitations (>10mm/h), and snowfall (any snowfall > 0in) will increase taxi demand. We think these will play a significant role because people generally don’t want to walk in these conditions (as mentioned in [4] above).\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "We use three datasets that together allow us to model NYC taxi demand as a function of time, location, and weather:\n",
    "\n",
    "- **Dataset 1 — NYC Yellow Taxi Trip Records (Dec 2024–Feb 2025)**\n",
    "  - Source: [NYC Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "  - Observations: ~11 million trips (3 monthly parquet files)\n",
    "  - Variables: 20 columns including pickup/dropoff timestamps, location zone IDs, passenger count, trip distance, fare amount, and surcharges\n",
    "  - Key variables: `tpep_pickup_datetime`, `PULocationID`, `passenger_count`, `trip_distance`, `fare_amount`, `total_amount`\n",
    "  - Shortcomings: ~25% of rows have null `passenger_count`. Some trips have dates outside the expected month. No rider demographics.\n",
    "\n",
    "- **Dataset 2 — NOAA Daily Weather Summaries (Dec 2024–Feb 2025)**\n",
    "  - Source: [NOAA Climate Data Online](https://www.ncdc.noaa.gov/cdo-web/) — Station USW00094728 (Central Park, NYC)\n",
    "  - Observations: 90 daily records\n",
    "  - Variables: 6 columns — `date`, `temp_max_f`, `temp_min_f`, `precipitation_in`, `snowfall_in`, `snow_on_ground_in`\n",
    "  - Parsed from official NOAA PDF reports.\n",
    "  - Shortcomings: Single weather station for all of NYC; actual weather may be different in other boroughs.\n",
    "\n",
    "- **Dataset 3 — NYC Taxi Zone Lookup**\n",
    "  - Source: [NYC TLC](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "  - Observations: 265 zones\n",
    "  - Variables: `LocationID`, `Borough`, `Zone`, `service_zone`\n",
    "  - Maps numeric zone IDs in the trip data to human-readable borough and zone names.\n",
    "\n",
    "**Combining the datasets:** Taxi trips are joined to the zone lookup on `PULocationID = LocationID` to add borough/zone names. A `date` column extracted from pickup timestamps joins trips with daily weather. The final merged dataset has one row per trip with added borough/zone names and that day’s weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/thiagodonato/anaconda3/envs/COGS108_FA25/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in /Users/thiagodonato/anaconda3/envs/COGS108_FA25/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/thiagodonato/anaconda3/envs/COGS108_FA25/lib/python3.11/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thiagodonato/anaconda3/envs/COGS108_FA25/lib/python3.11/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thiagodonato/anaconda3/envs/COGS108_FA25/lib/python3.11/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thiagodonato/anaconda3/envs/COGS108_FA25/lib/python3.11/site-packages (from requests) (2026.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:  20%|██        | 1/5 [00:03<00:13,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: yellow_tripdata_2024-12.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:  40%|████      | 2/5 [00:06<00:10,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: yellow_tripdata_2025-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:  60%|██████    | 3/5 [00:10<00:07,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: yellow_tripdata_2025-02.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:  80%|████████  | 4/5 [00:11<00:02,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: weather.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress: 100%|██████████| 5/5 [00:13<00:00,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: taxi_zone_lookup.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "%pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://drive.google.com/uc?id=1uqW6HLQWbky5cZA_OmJGp1VRdFCcWrSY', 'filename': 'yellow_tripdata_2024-12.parquet' },\n",
    "    { 'url': 'https://drive.google.com/uc?id=1TdzWDei4EQNvA192MRh9ViAAGd9Om_hp', 'filename': 'yellow_tripdata_2025-01.parquet' },\n",
    "    { 'url': 'https://drive.google.com/uc?id=1Csr-9K8WW4kjpdo3ile8dcnk4NsIJ9ic', 'filename': 'yellow_tripdata_2025-02.parquet' },\n",
    "    { 'url': 'https://drive.google.com/uc?id=1n2nCwsrrD-z_K12KgGtPBmtzQQ4uvTUS', 'filename': 'weather.csv' },\n",
    "    { 'url': 'https://drive.google.com/uc?id=1EBxlK--zYYWDTyqdPKF903ShGRvu5bjf', 'filename': 'taxi_zone_lookup.csv' },\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: NYC Yellow Taxi Trip Records (Dec 2024–Feb 2025)\n",
    "\n",
    "The NYC Taxi and Limousine Commission publishes monthly trip-level records for all yellow taxi rides. Each row is a single metered trip:\n",
    "\n",
    "- **tpep_pickup_datetime / tpep_dropoff_datetime** — Timestamps recorded by the taximeter, defining when and how long each trip lasted.\n",
    "- **PULocationID / DOLocationID** — Integer codes (1–265) identifying the taxi zone for pickup and dropoff. These map to the zone lookup table.\n",
    "- **passenger_count** — Self-reported by the driver; ranges 0–9 but most trips carry 1–2. About 25% of records have this field null (all from VendorID 1), meaning it is missing systematically by vendor, not at random. Could be a red flag for VendorID 1, and maybe best to remove these rows.\n",
    "- **trip_distance** — Distance in miles from the taximeter. Zero values indicate cancelled trips or meter errors so we need to remove those\n",
    "- **fare_amount** — Base fare in USD. Negative values occasionally appear from adjustments.\n",
    "- **total_amount** — Fare + surcharges + tips + tolls. What the rider actually paid.\n",
    "\n",
    "**Concerns:** Some trips have dates outside the nominal month (e.g., Nov 30 in the December file). Trips with 0 passengers, 0 distance, or negative fares are meter errors or cancellations. LocationIDs 264 and 265 correspond to \"Unknown\" zones. All of these are filtered during cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (10721140, 20)\n",
      "\n",
      "Column dtypes:\n",
      "VendorID                          int32\n",
      "tpep_pickup_datetime     datetime64[us]\n",
      "tpep_dropoff_datetime    datetime64[us]\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "RatecodeID                      float64\n",
      "store_and_fwd_flag               object\n",
      "PULocationID                      int32\n",
      "DOLocationID                      int32\n",
      "payment_type                      int64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "congestion_surcharge            float64\n",
      "Airport_fee                     float64\n",
      "cbd_congestion_fee              float64\n",
      "dtype: object\n",
      "\n",
      "Null counts:\n",
      "VendorID                       0\n",
      "tpep_pickup_datetime           0\n",
      "tpep_dropoff_datetime          0\n",
      "passenger_count          1673377\n",
      "trip_distance                  0\n",
      "RatecodeID               1673377\n",
      "store_and_fwd_flag       1673377\n",
      "PULocationID                   0\n",
      "DOLocationID                   0\n",
      "payment_type                   0\n",
      "fare_amount                    0\n",
      "extra                          0\n",
      "mta_tax                        0\n",
      "tip_amount                     0\n",
      "tolls_amount                   0\n",
      "improvement_surcharge          0\n",
      "total_amount                   0\n",
      "congestion_surcharge     1673377\n",
      "Airport_fee              1673377\n",
      "cbd_congestion_fee       3668371\n",
      "dtype: int64\n",
      "\n",
      "Date range: 2008-12-31 23:03:59 to 2025-03-23 20:42:06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load raw taxi trip files\n",
    "taxi_files = [\n",
    "    'data/00-raw/yellow_tripdata_2024-12.parquet',\n",
    "    'data/00-raw/yellow_tripdata_2025-01.parquet',\n",
    "    'data/00-raw/yellow_tripdata_2025-02.parquet',\n",
    "]\n",
    "# put all months together in one df\n",
    "taxi_raw = pd.concat([pd.read_parquet(f) for f in taxi_files], ignore_index=True)\n",
    "\n",
    "print(f\"Raw shape: {taxi_raw.shape}\")\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "print(taxi_raw.dtypes)\n",
    "print(f\"\\nNull counts:\")\n",
    "print(taxi_raw.isnull().sum())\n",
    "print(f\"\\nDate range: {taxi_raw['tpep_pickup_datetime'].min()} to {taxi_raw['tpep_pickup_datetime'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with nulls: ['passenger_count', 'RatecodeID', 'store_and_fwd_flag', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee']\n",
      "\n",
      "Null passenger_count by VendorID:\n",
      "VendorID\n",
      "1     267538\n",
      "2    1404606\n",
      "6       1233\n",
      "7          0\n",
      "Name: passenger_count, dtype: int64\n",
      "\n",
      "This confirms nulls come entirely from VendorID 1 (not missing at random).\n"
     ]
    }
   ],
   "source": [
    "# check if any columns are null\n",
    "null_cols = taxi_raw.columns[taxi_raw.isnull().any()]\n",
    "print(\"Columns with nulls:\", list(null_cols))\n",
    "\n",
    "# are nulls random or are they all from the same vendor?\n",
    "print(\"\\nNull passenger_count by VendorID:\")\n",
    "print(taxi_raw.groupby('VendorID')['passenger_count'].apply(lambda x: x.isnull().sum()))\n",
    "print(\"\\nThis confirms nulls come entirely from VendorID 1 (not missing at random).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After date filter: 10,721,108 rows (removed 32)\n",
      "After removing unknown zones: 10,691,219 rows (removed 29,889)\n",
      "After removing non-positive fares: 10,281,792 rows (removed 409,427)\n",
      "After removing fare outliers (>$500): 10,281,660 rows (removed 132)\n",
      "After removing dist=0 trips: 10,057,973 rows (removed 223,687)\n",
      "\n",
      "Null passenger_count before fill: 1,330,827\n",
      "Final taxi shape: (10057973, 11)\n",
      "\n",
      "Date range: 2024-12-01 to 2025-02-28\n",
      "\n",
      "Remaining nulls:\n",
      "tpep_pickup_datetime     0\n",
      "tpep_dropoff_datetime    0\n",
      "passenger_count          0\n",
      "trip_distance            0\n",
      "PULocationID             0\n",
      "DOLocationID             0\n",
      "fare_amount              0\n",
      "total_amount             0\n",
      "date                     0\n",
      "hour                     0\n",
      "day_of_week              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# cleaning the data into taxi dataframe\n",
    "taxi = taxi_raw.copy()\n",
    "n_before = len(taxi)\n",
    "\n",
    "# filter to Dec 1, 2024 – Feb 28, 2025 only (exclude out of intended sample dates)\n",
    "taxi = taxi[\n",
    "    (taxi['tpep_pickup_datetime'] >= '2024-12-01') &\n",
    "    (taxi['tpep_pickup_datetime'] < '2025-03-01')\n",
    "]\n",
    "print(f\"After date filter: {len(taxi):,} rows (removed {n_before - len(taxi):,})\")\n",
    "\n",
    "# drop rows with unknown pickup zones (264, 265)\n",
    "n = len(taxi)\n",
    "taxi = taxi[~taxi['PULocationID'].isin([264, 265])]\n",
    "print(f\"After removing unknown zones: {len(taxi):,} rows (removed {n - len(taxi):,})\")\n",
    "\n",
    "# removing errnoeus trips with zero or negative fare\n",
    "n = len(taxi)\n",
    "taxi = taxi[taxi['fare_amount'] > 0]\n",
    "print(f\"After removing non-positive fares: {len(taxi):,} rows (removed {n - len(taxi):,})\")\n",
    "\n",
    "# remove extreme fare outliers (> $500) - could be errors, sounds sketchy\n",
    "n = len(taxi)\n",
    "taxi = taxi[taxi['fare_amount'] <= 500]\n",
    "print(f\"After removing fare outliers (>$500): {len(taxi):,} rows (removed {n - len(taxi):,})\")\n",
    "\n",
    "# remove cancelled/zero-distance trips\n",
    "n = len(taxi)\n",
    "taxi = taxi[taxi['trip_distance'] > 0]\n",
    "print(f\"After removing dist=0 trips: {len(taxi):,} rows (removed {n - len(taxi):,})\")\n",
    "\n",
    "# replace null passenger_count with 1 (most common value) - just to have data clean.\n",
    "# we are not using passenger_count for taxi demand analysis yet, so its okay to fill it so the data is clean\n",
    "print(f\"\\nNull passenger_count before fill: {taxi['passenger_count'].isnull().sum():,}\")\n",
    "taxi['passenger_count'] = taxi['passenger_count'].fillna(1.0)\n",
    "\n",
    "# get temporal features (change names to something more readable)\n",
    "taxi['date'] = taxi['tpep_pickup_datetime'].dt.date.astype(str)\n",
    "taxi['hour'] = taxi['tpep_pickup_datetime'].dt.hour\n",
    "taxi['day_of_week'] = taxi['tpep_pickup_datetime'].dt.day_name()\n",
    "\n",
    "# keep only columns we might be interested in\n",
    "keep_cols = [\n",
    "    'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "    'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID',\n",
    "    'fare_amount', 'total_amount', 'date', 'hour', 'day_of_week'\n",
    "]\n",
    "taxi = taxi[keep_cols]\n",
    "\n",
    "print(f\"Final taxi shape: {taxi.shape}\")\n",
    "print(f\"\\nDate range: {taxi['date'].min()} to {taxi['date'].max()}\")\n",
    "print(f\"\\nRemaining nulls:\\n{taxi.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for cleaned taxi data:\n",
      "\n",
      "       passenger_count  trip_distance  fare_amount  total_amount\n",
      "count      10057973.00    10057973.00  10057973.00   10057973.00\n",
      "mean              1.27           5.49        18.81         27.62\n",
      "std               0.72         503.86        16.58         20.88\n",
      "min               0.00           0.01         0.01          0.00\n",
      "25%               1.00           1.01         9.30         16.02\n",
      "50%               1.00           1.71        13.50         21.00\n",
      "75%               1.00           3.22        21.20         29.75\n",
      "max               9.00      328827.63       500.00        615.31\n",
      "\n",
      "Trips per day: mean=111755.25555555556, min=52112, max=153849\n"
     ]
    }
   ],
   "source": [
    "# summary statistics for our cleaned data\n",
    "print('Summary statistics for cleaned taxi data:\\n')\n",
    "print(taxi[['passenger_count', 'trip_distance', 'fare_amount', 'total_amount']].describe().round(2))\n",
    "\n",
    "trips_per_day = taxi.groupby('date').size()\n",
    "print(f'\\nTrips per day: mean={trips_per_day.mean()}, min={trips_per_day.min()}, max={trips_per_day.max()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: NOAA Daily Weather — Central Park (Dec 2024–Feb 2025)\n",
    "\n",
    "Daily weather summaries from NOAA station USW00094728 (Central Park, Manhattan). This is the standard reference station for NYC weather, recording continuously since 1869.\n",
    "\n",
    "- **temp_max_f / temp_min_f** — Daily high and low temperatures in degrees Fahrenheit. For Dec 2024–Feb 2025, highs range from the low 40s (late November) to the mid-80s (early September).\n",
    "- **precipitation_in** — Total liquid precipitation (rain + melted snow) in inches per 24-hour period. Values of 0.001 represent \"trace\" amounts (originally \"T\" in the source data).\n",
    "- **snowfall_in** — New snowfall in inches. Winter months include multiple snow events.\n",
    "- **snow_on_ground_in** — Snow depth on the ground. Non-zero after snowfall events.\n",
    "\n",
    "**Concerns:** Central Park is a single station for an entire metro area. Conditions in the outer boroughs or near airports may differ, especially for localized precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather shape: (90, 6)\n",
      "Date range: 2024-12-01 to 2025-02-28\n",
      "\n",
      "Data types:\n",
      "date                  object\n",
      "temp_max_f           float64\n",
      "temp_min_f           float64\n",
      "precipitation_in     float64\n",
      "snowfall_in          float64\n",
      "snow_on_ground_in    float64\n",
      "dtype: object\n",
      "\n",
      "Null counts:\n",
      "date                 0\n",
      "temp_max_f           0\n",
      "temp_min_f           0\n",
      "precipitation_in     0\n",
      "snowfall_in          0\n",
      "snow_on_ground_in    0\n",
      "dtype: int64\n",
      "\n",
      "Descriptive statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_max_f</th>\n",
       "      <th>temp_min_f</th>\n",
       "      <th>precipitation_in</th>\n",
       "      <th>snowfall_in</th>\n",
       "      <th>snow_on_ground_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.311111</td>\n",
       "      <td>29.311111</td>\n",
       "      <td>0.086067</td>\n",
       "      <td>0.143411</td>\n",
       "      <td>0.362289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.202643</td>\n",
       "      <td>8.603167</td>\n",
       "      <td>0.187261</td>\n",
       "      <td>0.453191</td>\n",
       "      <td>0.702221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46.750000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temp_max_f  temp_min_f  precipitation_in  snowfall_in  \\\n",
       "count   90.000000   90.000000         90.000000    90.000000   \n",
       "mean    40.311111   29.311111          0.086067     0.143411   \n",
       "std      9.202643    8.603167          0.187261     0.453191   \n",
       "min     19.000000   10.000000          0.000000     0.000000   \n",
       "25%     34.000000   24.000000          0.000000     0.000000   \n",
       "50%     39.000000   29.000000          0.000000     0.000000   \n",
       "75%     46.750000   33.000000          0.087500     0.000000   \n",
       "max     60.000000   50.000000          0.910000     3.000000   \n",
       "\n",
       "       snow_on_ground_in  \n",
       "count          90.000000  \n",
       "mean            0.362289  \n",
       "std             0.702221  \n",
       "min             0.000000  \n",
       "25%             0.000000  \n",
       "50%             0.000000  \n",
       "75%             0.001000  \n",
       "max             3.100000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the weather data\n",
    "weather = pd.read_csv('data/00-raw/weather.csv')\n",
    "\n",
    "print(f\"Weather shape: {weather.shape}\")\n",
    "print(f\"Date range: {weather['date'].iloc[0]} to {weather['date'].iloc[-1]}\")\n",
    "print(f\"\\nData types:\\n{weather.dtypes}\")\n",
    "print(f\"\\nNull counts:\\n{weather.isnull().sum()}\")\n",
    "print(f\"\\nDescriptive statistics:\")\n",
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing days in weather data: 0 (expected 0)\n",
      "Remaining nulls: 0\n",
      "\n",
      "Cleaned weather shape: (90, 6)\n"
     ]
    }
   ],
   "source": [
    "# cleaning it\n",
    "# filling any nulls in snow_on_ground_in with 0\n",
    "weather['snow_on_ground_in'] = weather['snow_on_ground_in'].fillna(0.0)\n",
    "\n",
    "# The code below on this cell was partially generated by Claude Opus from the prompt \n",
    "# \"assume we have weather dataframe with col 'date_parsed' that is pd.datetime\n",
    "# write code that checks how many missing days there are. We expect to have one\n",
    "# day for each day in the range 2024-12-01 to 2025-02-28.\"\n",
    "\n",
    "# check that there are no gaps on days\n",
    "weather['date_parsed'] = pd.to_datetime(weather['date'])\n",
    "expected_days = pd.date_range('2024-12-01', '2025-02-28', freq='D')\n",
    "missing_days = set(expected_days) - set(weather['date_parsed'])\n",
    "print(f'Missing days in weather data: {len(missing_days)} (expected 0)')\n",
    "print(f'Remaining nulls: {weather.isnull().sum().sum()}')\n",
    "\n",
    "weather = weather.drop(columns=['date_parsed'])\n",
    "print(f'\\nCleaned weather shape: {weather.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3: Taxi Zone Lookup + Merging All Datasets\n",
    "\n",
    "The zone lookup maps the integer `LocationID` in the trip data to borough and zone names. We join it to the taxi data on `PULocationID` to add `Borough`, `Zone`, and `service_zone`. Then we join weather data on `date` so every trip is enriched with that day’s weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zone lookup shape: (265, 4)\n",
      "Boroughs: ['EWR' 'Queens' 'Bronx' 'Manhattan' 'Staten Island' 'Brooklyn' 'Unknown'\n",
      " nan]\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone\n",
       "0           1            EWR           Newark Airport          EWR\n",
       "1           2         Queens              Jamaica Bay    Boro Zone\n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
       "3           4      Manhattan            Alphabet City  Yellow Zone\n",
       "4           5  Staten Island            Arden Heights    Boro Zone"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get zone data\n",
    "zones = pd.read_csv('data/00-raw/taxi_zone_lookup.csv')\n",
    "print(f\"Zone lookup shape: {zones.shape}\")\n",
    "print(f\"Boroughs: {zones['Borough'].unique()}\")\n",
    "print(f\"\\nSample:\")\n",
    "zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After zone join: (10057973, 14)\n",
      "Null Borough (unmatched zones): 0\n",
      "After weather join: (10057973, 19)\n",
      "\n",
      "Final merged dataset: 10,057,973 rows x 19 columns\n",
      "\n",
      "Column list: ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'total_amount', 'date', 'hour', 'day_of_week', 'Borough', 'Zone', 'service_zone', 'temp_max_f', 'temp_min_f', 'precipitation_in', 'snowfall_in', 'snow_on_ground_in']\n",
      "\n",
      "Remaining nulls:\n",
      "tpep_pickup_datetime     0\n",
      "tpep_dropoff_datetime    0\n",
      "passenger_count          0\n",
      "trip_distance            0\n",
      "PULocationID             0\n",
      "DOLocationID             0\n",
      "fare_amount              0\n",
      "total_amount             0\n",
      "date                     0\n",
      "hour                     0\n",
      "day_of_week              0\n",
      "Borough                  0\n",
      "Zone                     0\n",
      "service_zone             0\n",
      "temp_max_f               0\n",
      "temp_min_f               0\n",
      "precipitation_in         0\n",
      "snowfall_in              0\n",
      "snow_on_ground_in        0\n",
      "dtype: int64\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "      <th>temp_max_f</th>\n",
       "      <th>temp_min_f</th>\n",
       "      <th>precipitation_in</th>\n",
       "      <th>snowfall_in</th>\n",
       "      <th>snow_on_ground_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-01 00:12:27</td>\n",
       "      <td>2024-12-01 00:31:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.76</td>\n",
       "      <td>138</td>\n",
       "      <td>33</td>\n",
       "      <td>38.0</td>\n",
       "      <td>51.97</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Queens</td>\n",
       "      <td>LaGuardia Airport</td>\n",
       "      <td>Airports</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-01 00:50:35</td>\n",
       "      <td>2024-12-01 01:24:46</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.07</td>\n",
       "      <td>132</td>\n",
       "      <td>236</td>\n",
       "      <td>70.0</td>\n",
       "      <td>82.69</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Queens</td>\n",
       "      <td>JFK Airport</td>\n",
       "      <td>Airports</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-01 00:18:16</td>\n",
       "      <td>2024-12-01 00:33:16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.34</td>\n",
       "      <td>142</td>\n",
       "      <td>186</td>\n",
       "      <td>15.6</td>\n",
       "      <td>24.72</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Lincoln Square East</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-01 00:56:13</td>\n",
       "      <td>2024-12-01 01:18:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>107</td>\n",
       "      <td>80</td>\n",
       "      <td>26.8</td>\n",
       "      <td>36.80</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Gramercy</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-01 00:21:17</td>\n",
       "      <td>2024-12-01 00:37:22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>249</td>\n",
       "      <td>141</td>\n",
       "      <td>20.5</td>\n",
       "      <td>30.60</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>West Village</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
       "0  2024-12-01 00:12:27   2024-12-01 00:31:12              1.0           9.76   \n",
       "1  2024-12-01 00:50:35   2024-12-01 01:24:46              4.0          20.07   \n",
       "2  2024-12-01 00:18:16   2024-12-01 00:33:16              3.0           2.34   \n",
       "3  2024-12-01 00:56:13   2024-12-01 01:18:25              1.0           5.05   \n",
       "4  2024-12-01 00:21:17   2024-12-01 00:37:22              1.0           4.30   \n",
       "\n",
       "   PULocationID  DOLocationID  fare_amount  total_amount        date  hour  \\\n",
       "0           138            33         38.0         51.97  2024-12-01     0   \n",
       "1           132           236         70.0         82.69  2024-12-01     0   \n",
       "2           142           186         15.6         24.72  2024-12-01     0   \n",
       "3           107            80         26.8         36.80  2024-12-01     0   \n",
       "4           249           141         20.5         30.60  2024-12-01     0   \n",
       "\n",
       "  day_of_week    Borough                 Zone service_zone  temp_max_f  \\\n",
       "0      Sunday     Queens    LaGuardia Airport     Airports        38.0   \n",
       "1      Sunday     Queens          JFK Airport     Airports        38.0   \n",
       "2      Sunday  Manhattan  Lincoln Square East  Yellow Zone        38.0   \n",
       "3      Sunday  Manhattan             Gramercy  Yellow Zone        38.0   \n",
       "4      Sunday  Manhattan         West Village  Yellow Zone        38.0   \n",
       "\n",
       "   temp_min_f  precipitation_in  snowfall_in  snow_on_ground_in  \n",
       "0        27.0               0.0          0.0                0.0  \n",
       "1        27.0               0.0          0.0                0.0  \n",
       "2        27.0               0.0          0.0                0.0  \n",
       "3        27.0               0.0          0.0                0.0  \n",
       "4        27.0               0.0          0.0                0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge taxi + zone + weather\n",
    "# join taxi with zone lookup based on PULocationID\n",
    "merged = taxi.merge(\n",
    "    zones, left_on='PULocationID', right_on='LocationID', how='left'\n",
    ").drop(columns=['LocationID'])\n",
    "\n",
    "print(f\"After zone join: {merged.shape}\")\n",
    "print(f\"Null Borough (unmatched zones): {merged['Borough'].isnull().sum()}\")\n",
    "\n",
    "# join with weather on date\n",
    "merged = merged.merge(weather, on='date', how='left')\n",
    "print(f\"After weather join: {merged.shape}\")\n",
    "\n",
    "# final checks and sanity prints\n",
    "print(f\"\\nFinal merged dataset: {merged.shape[0]:,} rows x {merged.shape[1]} columns\")\n",
    "print(f\"\\nColumn list: {list(merged.columns)}\")\n",
    "print(f\"\\nRemaining nulls:\\n{merged.isnull().sum()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged dataset to data/02-processed/taxi_weather_merged.parquet\n",
      "Saved cleaned taxi to data/01-interim/taxi_cleaned.parquet\n",
      "Saved cleaned weathe to data/01-interim/weather_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# save changes to processed dir\n",
    "merged.to_parquet('data/02-processed/taxi_weather_merged.parquet', index=False)\n",
    "print(f\"Saved merged dataset to data/02-processed/taxi_weather_merged.parquet\")\n",
    "\n",
    "# save cleaned individual data to their interim dirs\n",
    "taxi.to_parquet('data/01-interim/taxi_cleaned.parquet', index=False)\n",
    "weather.to_csv('data/01-interim/weather_cleaned.csv', index=False)\n",
    "print(f\"Saved cleaned taxi to data/01-interim/taxi_cleaned.parquet\")\n",
    "print(f\"Saved cleaned weathe to data/01-interim/weather_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics \n",
    "\n",
    "Instructions: Keep the contents of this cell. For each item on the checklist\n",
    "-  put an X there if you've considered the item\n",
    "-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
    "  \n",
    "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
    "\n",
    "Here is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n",
    "\n",
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent? \n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those? \n",
    " - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    " - [ ] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [ ] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)? \n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)? \n",
    " - [ ] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [ ] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory? X \n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)? X \n",
    " - [ ] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [ ] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "\n",
    "Notes on ethical considerations:\n",
    "\n",
    "The TLC trip data is collected without explicit rider consent, but does not have identifiable information about riders (A.1). Also it only covers TLC-regulated vehicles, leaving out public transit, biking, and personal cars (A.2). There are missing perspectives and dataset bias because we do not know anything about the riders (including reason for getting a taxi), and Manhattan is likely to skew the results due to being a densely populated area (C.1, C.2). Location features like pick-up and drop-off zones can also proxy for race and income due to NYC's geographical segregation (D.1, D.2).\n",
    "\n",
    "As for location, the TLC's dataset divides NYC into 263 zones, which protects passenger privacy by avoiding exact GPS coordinates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work\n",
    "  \n",
    "Read over the [COGS108 Team Policies](https://github.com/COGS108/Projects/blob/master/COGS108_TeamPolicies.md) individually. Then, include your group’s expectations of one another for successful completion of your COGS108 project below. Discuss and agree on what all of your expectations are. Discuss how your team will communicate throughout the quarter and consider how you will communicate respectfully should conflicts arise. By including each member’s name above and by adding their name to the submission, you are indicating that you have read the COGS108 Team Policies, accept your team’s expectations below, and have every intention to fulfill them. These expectations are for your team’s use and benefit — they won’t be graded for their details.\n",
    "\n",
    "* Communicate using our Discord group chat.\n",
    "* Reasonable to wait around 3-7 hours for a message to be replied to.\n",
    "* We will meet once a week, on either Tuesday or Thursday, in person.\n",
    "* Blunt but polite tone encouraged.\n",
    "* Majority vote is how we will deal with things.\n",
    "* Everyone will do a little bit of everything with tasks\n",
    "* Keep track of everything using Google Sheets\n",
    "* If someone is struggling, they should notify the group at least 72 hours after they have been struggling to find data information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date | Meeting Time | Completed Before Meeting | Discuss at Meeting |\n",
    "|-------------|--------------|--------------------------|--------------------|\n",
    "| 2/4 | 8 pm | Reading and Discussing COGS 108 expectations. | Brainstorm topics for final project and decide on idea; discuss hypothesis; begin background research; submit project proposal. <br>Content covered in meeting:<br>- Decided on final project idea and research question: Out of specific features between temporal, spatial, and weather, which feature or combination of features has the most important effect on taxi-demands in New York City?<br>- Began brainstorming hypothesis.<br>- Background research was mostly completed during the meeting with data sets found, pending selection of ideal data sets. |\n",
    "| 2/10 | 8 pm | Background research on topic; search for data sets. | Discuss ideal data sets; discuss wrangling and possible analytical approaches; assign roles for each group member to lead a specific part. <br>Content covered in meeting:<br>- Refined hypothesis to match and relate to our data sets.<br>- Discussed and found ideal data sets to use for the final project.<br>- Began wrangling data for ideal data sets. |\n",
    "| 2/19 | 8 pm | Import and Wrangle Data; EDA | Review/Edit wrangling and EDA; discuss analysis plan. Decide on five pairs of features (one for each member) to dig deeper, including potential confounding variables and correlations with taxi demand, based on the correlation matrix of all variables. |\n",
    "| 2/26 | 8 pm | Finalize wrangling/EDA; Begin Analysis | Discuss/edit analysis with focus on preliminary visualizations and model development/comparison based on feature selection from the previous meeting; complete project check-in. |\n",
    "| 3/12 | 8 pm | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project with focus on finalizing plots, interpreting results, comparing model outcomes, and refining conclusions and final submission. Test selected features. |\n",
    "| 3/20 | Before 11:59 pm | NA | Turn in Final Project and Group Project Surveys |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS108_FA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
